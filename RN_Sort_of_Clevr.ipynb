{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RN_Sort_of_Clevr.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "kBaOoom4_Ol1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Dataset Generation**"
      ]
    },
    {
      "metadata": {
        "id": "9pGDArcS_JzL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6e3a2124-22b4-4299-c87e-d6142bd31097"
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "train_size = 9800\n",
        "test_size = 200\n",
        "img_size = 75\n",
        "size = 5\n",
        "question_size = 11 ##6 for one-hot vector of color, 2 for question type, 3 for question subtype\n",
        "\"\"\"Answer : [yes, no, rectangle, circle, r, g, b, o, k, y]\"\"\"\n",
        "\n",
        "nb_questions = 10\n",
        "dirs = './data'\n",
        "\n",
        "colors = [\n",
        "    (0,0,255),##r\n",
        "    (0,255,0),##g\n",
        "    (255,0,0),##b\n",
        "    (0,156,255),##o\n",
        "    (128,128,128),##k\n",
        "    (0,255,255)##y\n",
        "]\n",
        "\n",
        "\n",
        "try:\n",
        "    os.makedirs(dirs)\n",
        "except:\n",
        "    print('directory {} already exists'.format(dirs))\n",
        "\n",
        "def center_generate(objects):\n",
        "    while True:\n",
        "        pas = True\n",
        "        center = np.random.randint(0+size, img_size - size, 2)        \n",
        "        if len(objects) > 0:\n",
        "            for name,c,shape in objects:\n",
        "                if ((center - c) ** 2).sum() < ((size * 2) ** 2):\n",
        "                    pas = False\n",
        "        if pas:\n",
        "            return center\n",
        "\n",
        "\n",
        "\n",
        "def build_dataset():\n",
        "    objects = []\n",
        "    img = np.ones((img_size,img_size,3)) * 255\n",
        "    for color_id,color in enumerate(colors):  \n",
        "        center = center_generate(objects)\n",
        "        if random.random()<0.5:\n",
        "            start = (center[0]-size, center[1]-size)\n",
        "            end = (center[0]+size, center[1]+size)\n",
        "            cv2.rectangle(img, start, end, color, -1)\n",
        "            objects.append((color_id,center,'r'))\n",
        "        else:\n",
        "            center_ = (center[0], center[1])\n",
        "            cv2.circle(img, center_, size, color, -1)\n",
        "            objects.append((color_id,center,'c'))\n",
        "\n",
        "\n",
        "    rel_questions = []\n",
        "    norel_questions = []\n",
        "    rel_answers = []\n",
        "    norel_answers = []\n",
        "    \"\"\"Non-relational questions\"\"\"\n",
        "    for _ in range(nb_questions):\n",
        "        question = np.zeros((question_size))\n",
        "        color = random.randint(0,5)\n",
        "        question[color] = 1\n",
        "        question[6] = 1\n",
        "        subtype = random.randint(0,2)\n",
        "        question[subtype+8] = 1\n",
        "        norel_questions.append(question)\n",
        "        \"\"\"Answer : [yes, no, rectangle, circle, r, g, b, o, k, y]\"\"\"\n",
        "        if subtype == 0:\n",
        "            \"\"\"query shape->rectangle/circle\"\"\"\n",
        "            if objects[color][2] == 'r':\n",
        "                answer = 2\n",
        "            else:\n",
        "                answer = 3\n",
        "\n",
        "        elif subtype == 1:\n",
        "            \"\"\"query horizontal position->yes/no\"\"\"\n",
        "            if objects[color][1][0] < img_size / 2:\n",
        "                answer = 0\n",
        "            else:\n",
        "                answer = 1\n",
        "\n",
        "        elif subtype == 2:\n",
        "            \"\"\"query vertical position->yes/no\"\"\"\n",
        "            if objects[color][1][1] < img_size / 2:\n",
        "                answer = 0\n",
        "            else:\n",
        "                answer = 1\n",
        "        norel_answers.append(answer)\n",
        "    \n",
        "    \"\"\"Relational questions\"\"\"\n",
        "    for i in range(nb_questions):\n",
        "        question = np.zeros((question_size))\n",
        "        color = random.randint(0,5)\n",
        "        question[color] = 1\n",
        "        question[7] = 1\n",
        "        subtype = random.randint(0,2)\n",
        "        question[subtype+8] = 1\n",
        "        rel_questions.append(question)\n",
        "\n",
        "        if subtype == 0:\n",
        "            \"\"\"closest-to->rectangle/circle\"\"\"\n",
        "            my_obj = objects[color][1]\n",
        "            dist_list = [((my_obj - obj[1]) ** 2).sum() for obj in objects]\n",
        "            dist_list[dist_list.index(0)] = 999\n",
        "            closest = dist_list.index(min(dist_list))\n",
        "            if objects[closest][2] == 'r':\n",
        "                answer = 2\n",
        "            else:\n",
        "                answer = 3\n",
        "                \n",
        "        elif subtype == 1:\n",
        "            \"\"\"furthest-from->rectangle/circle\"\"\"\n",
        "            my_obj = objects[color][1]\n",
        "            dist_list = [((my_obj - obj[1]) ** 2).sum() for obj in objects]\n",
        "            furthest = dist_list.index(max(dist_list))\n",
        "            if objects[furthest][2] == 'r':\n",
        "                answer = 2\n",
        "            else:\n",
        "                answer = 3\n",
        "\n",
        "        elif subtype == 2:\n",
        "            \"\"\"count->1~6\"\"\"\n",
        "            my_obj = objects[color][2]\n",
        "            count = -1\n",
        "            for obj in objects:\n",
        "                if obj[2] == my_obj:\n",
        "                    count +=1 \n",
        "            answer = count+4\n",
        "\n",
        "        rel_answers.append(answer)\n",
        "\n",
        "    relations = (rel_questions, rel_answers)\n",
        "    norelations = (norel_questions, norel_answers)\n",
        "    \n",
        "    img = img/255.\n",
        "    dataset = (img, relations, norelations)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "print('building test datasets...')\n",
        "test_datasets = [build_dataset() for _ in range(test_size)]\n",
        "print('building train datasets...')\n",
        "train_datasets = [build_dataset() for _ in range(train_size)]\n",
        "\n",
        "\n",
        "#img_count = 0\n",
        "#cv2.imwrite(os.path.join(dirs,'{}.png'.format(img_count)), cv2.resize(train_datasets[0][0]*255, (512,512)))\n",
        "\n",
        "\n",
        "print('saving datasets...')\n",
        "filename = os.path.join(dirs,'sort-of-clevr.pickle')\n",
        "with  open(filename, 'wb') as f:\n",
        "    pickle.dump((train_datasets, test_datasets), f)\n",
        "print('datasets saved at {}'.format(filename))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "building test datasets...\n",
            "building train datasets...\n",
            "saving datasets...\n",
            "datasets saved at ./data/sort-of-clevr.pickle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z5HfQWyl_W0n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **RN Model**"
      ]
    },
    {
      "metadata": {
        "id": "htdpm_nZ_ZYM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class ConvInputModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvInputModel, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, stride=2, padding=1)#input channel,output channel,kernel size\n",
        "        self.batchNorm1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, stride=2, padding=1)\n",
        "        self.batchNorm2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, stride=2, padding=1)\n",
        "        self.batchNorm3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, stride=2, padding=1)\n",
        "        self.batchNorm4 = nn.BatchNorm2d(256)\n",
        "\n",
        "        \n",
        "    def forward(self, img):\n",
        "        \"\"\"convolution\"\"\"\n",
        "        x = self.conv1(img)\n",
        "        x = F.relu(x)\n",
        "        x = self.batchNorm1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.batchNorm2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.batchNorm3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.batchNorm4(x)\n",
        "        return x\n",
        "\n",
        "  \n",
        "class FCOutputModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FCOutputModel, self).__init__()\n",
        "\n",
        "        self.fc2 = nn.Linear(1000, 500)\n",
        "        self.fc3 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x)\n",
        "\n",
        "class FCOutputModel2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FCOutputModel2, self).__init__()\n",
        "\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, 256)\n",
        "        self.fc4 = nn.Linear(256, 256)\n",
        "        self.fc5 = nn.Linear(256, 256)\n",
        "        self.fc6 = nn.Linear(256, 256)\n",
        "        self.fc7 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc4(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc5(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc6(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x)\n",
        "        x = self.fc7(x)\n",
        "        return F.log_softmax(x)  \n",
        "\n",
        "class BasicModel(nn.Module):\n",
        "    def __init__(self, args, name):\n",
        "        super(BasicModel, self).__init__()\n",
        "        self.name=name\n",
        "\n",
        "    def train_(self, input_img, input_qst, label):\n",
        "        self.optimizer.zero_grad()\n",
        "        output = self(input_img, input_qst)\n",
        "        loss = F.nll_loss(output, label)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        pred = output.data.max(1)[1]\n",
        "        correct = pred.eq(label.data).cpu().sum()\n",
        "        accuracy = correct * 100. / len(label)\n",
        "        return accuracy\n",
        "        \n",
        "    def test_(self, input_img, input_qst, label):\n",
        "        output = self(input_img, input_qst)\n",
        "        pred = output.data.max(1)[1]\n",
        "        correct = pred.eq(label.data).cpu().sum()\n",
        "        accuracy = correct * 100. / len(label)\n",
        "        return pred,accuracy\n",
        "\n",
        "    def save_model(self, epoch):\n",
        "        torch.save(self.state_dict(), 'model/epoch_{}_{:02d}.pth'.format(self.name, epoch))\n",
        "\n",
        "\n",
        "class RN(BasicModel):\n",
        "    def __init__(self, args):\n",
        "        super(RN, self).__init__(args, 'RN')\n",
        "        \n",
        "        self.conv = ConvInputModel()\n",
        "        \n",
        "        ##(number of filters per object+coordinate of object)*2+question vector\n",
        "        self.g_fc1 = nn.Linear((256+2)*2+11, 2000)\n",
        "\n",
        "        self.g_fc2 = nn.Linear(2000, 2000)\n",
        "        self.g_fc3 = nn.Linear(2000, 2000)\n",
        "        self.g_fc4 = nn.Linear(2000, 2000)\n",
        "\n",
        "        self.f_fc1 = nn.Linear(2000, 1000)\n",
        "\n",
        "        self.coord_oi = torch.FloatTensor(args.batch_size, 2)\n",
        "        self.coord_oj = torch.FloatTensor(args.batch_size, 2)\n",
        "        if args.cuda:\n",
        "            self.coord_oi = self.coord_oi.cuda()\n",
        "            self.coord_oj = self.coord_oj.cuda()\n",
        "        self.coord_oi = Variable(self.coord_oi)\n",
        "        self.coord_oj = Variable(self.coord_oj)\n",
        "\n",
        "        # # prepare coord tensor\n",
        "        # def cvt_coord(i):\n",
        "        #     return [(i/5-2)/2., (i%5-2)/2.]\n",
        "\n",
        "        # prepare coord tensor\n",
        "        def cvt_coord(i):\n",
        "            if i>=0 and i<=4:\n",
        "                tmp=[0,i]\n",
        "            elif i>=5 and i<=9:\n",
        "                tmp=[1,i%5]\n",
        "            elif i>=10 and i<=14:\n",
        "                tmp=[2,i%5]\n",
        "            elif i>=15 and i<=19:\n",
        "                tmp=[3,i%5]\n",
        "            elif i>=20 and i<=24:\n",
        "                tmp=[4,i%5]\n",
        "            return list((np.array(tmp)/2)-1)\n",
        "        \n",
        "        self.coord_tensor = torch.FloatTensor(args.batch_size, 25, 2)\n",
        "        if args.cuda:\n",
        "            self.coord_tensor = self.coord_tensor.cuda()\n",
        "        self.coord_tensor = Variable(self.coord_tensor)\n",
        "        np_coord_tensor = np.zeros((args.batch_size, 25, 2))\n",
        "        for i in range(25):\n",
        "            np_coord_tensor[:,i,:] = np.array( cvt_coord(i) )\n",
        "        self.coord_tensor.data.copy_(torch.from_numpy(np_coord_tensor))\n",
        "\n",
        "\n",
        "        self.fcout = FCOutputModel()\n",
        "        \n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=args.lr)\n",
        "\n",
        "\n",
        "    def forward(self, img, qst):\n",
        "        x = self.conv(img) ## x = (64 x 24 x 5 x 5)\n",
        "        \n",
        "        \"\"\"g\"\"\"\n",
        "        mb = x.size()[0]\n",
        "        n_channels = x.size()[1]\n",
        "        d = x.size()[2]\n",
        "        x_flat = x.view(mb,n_channels,d*d).permute(0,2,1)# x_flat = (64 x 25 x 24)\n",
        "        \n",
        "        # add coordinates\n",
        "        x_flat = torch.cat([x_flat, self.coord_tensor],2)# x_flat = (64 x 25 x 26)\n",
        "        \n",
        "        # add question everywhere\n",
        "        qst = torch.unsqueeze(qst, 1)\n",
        "        qst = qst.repeat(1,25,1)\n",
        "        qst = torch.unsqueeze(qst, 2)\n",
        "        \n",
        "        # cast all pairs against each other\n",
        "        x_i = torch.unsqueeze(x_flat,1) # (64x1x25x26)\n",
        "        x_i = x_i.repeat(1,25,1,1) # (64x25x25x26)\n",
        "        x_j = torch.unsqueeze(x_flat,2) # (64x25x1x26)\n",
        "        x_j = torch.cat([x_j,qst],3)# (64x25x1x(26+11))\n",
        "        x_j = x_j.repeat(1,1,25,1) # (64x25x25x(26+11))\n",
        "        \n",
        "        # concatenate all together\n",
        "        x_full = torch.cat([x_i,x_j],3) # (64x25x25x(26+26+11))\n",
        "        \n",
        "        # reshape for passing through network\n",
        "        x_ = x_full.view(mb*d*d*d*d,527) #63=26X2+11\n",
        "        x_ = self.g_fc1(x_)\n",
        "        x_ = F.relu(x_)\n",
        "        x_ = self.g_fc2(x_)\n",
        "        x_ = F.relu(x_)\n",
        "        x_ = self.g_fc3(x_)\n",
        "        x_ = F.relu(x_)\n",
        "        x_ = self.g_fc4(x_)\n",
        "        x_ = F.relu(x_)\n",
        "        \n",
        "        # reshape again and sum\n",
        "        x_g = x_.view(mb,d*d*d*d,2000)\n",
        "        x_g = x_g.sum(1).squeeze()\n",
        "        \n",
        "        \"\"\"f\"\"\"\n",
        "        x_f = self.f_fc1(x_g)\n",
        "        x_f = F.relu(x_f)\n",
        "        \n",
        "        return self.fcout(x_f)\n",
        "\n",
        "\n",
        "class CNN_MLP(BasicModel):\n",
        "    def __init__(self, args):\n",
        "        super(CNN_MLP, self).__init__(args, 'CNNMLP')\n",
        "\n",
        "        self.conv  = ConvInputModel()\n",
        "        self.fc1   = nn.Linear(5*5*24 + 11, 256)  # question concatenated to all\n",
        "        self.fcout = FCOutputModel2()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=args.lr)\n",
        "        #print([ a for a in self.parameters() ] )\n",
        "  \n",
        "    def forward(self, img, qst):\n",
        "        x = self.conv(img) ## x = (64 x 24 x 5 x 5)\n",
        "\n",
        "        \"\"\"fully connected layers\"\"\"\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        x_ = torch.cat((x, qst), 1)  # Concat question\n",
        "        \n",
        "        x_ = self.fc1(x_)\n",
        "        x_ = F.relu(x_)\n",
        "        \n",
        "        return self.fcout(x_)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sk9RiErK_keE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ]
    },
    {
      "metadata": {
        "id": "I1XYJkm4_nYD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "resumeFlag=None\n",
        "# resumeFlag='epoch_RN_20.pth'\n",
        "bs=64\n",
        "# bs=80\n",
        "mydict={'batch_size':bs,'cuda':True,'epochs':20,'log_interval':10,'lr':0.00025,'model':'RN','no_cuda':False,'resume':resumeFlag,'seed':1}\n",
        "args = Namespace(**mydict)\n",
        "model=RN(args)\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gilo_9fY_vp0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "#import cPickle as pickle\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "model_dirs = './model'\n",
        "bs = args.batch_size\n",
        "input_img = torch.FloatTensor(bs, 3, 75, 75)\n",
        "input_qst = torch.FloatTensor(bs, 11)\n",
        "label = torch.LongTensor(bs)\n",
        "\n",
        "if args.cuda:\n",
        "    model.cuda()\n",
        "    input_img = input_img.cuda()\n",
        "    input_qst = input_qst.cuda()\n",
        "    label = label.cuda()\n",
        "\n",
        "input_img = Variable(input_img)\n",
        "input_qst = Variable(input_qst)\n",
        "label = Variable(label)\n",
        "\n",
        "def tensor_data(data, i):\n",
        "    img = torch.from_numpy(np.asarray(data[0][bs*i:bs*(i+1)]))\n",
        "    qst = torch.from_numpy(np.asarray(data[1][bs*i:bs*(i+1)]))\n",
        "    ans = torch.from_numpy(np.asarray(data[2][bs*i:bs*(i+1)]))\n",
        "\n",
        "    input_img.data.resize_(img.size()).copy_(img)\n",
        "    input_qst.data.resize_(qst.size()).copy_(qst)\n",
        "    label.data.resize_(ans.size()).copy_(ans)\n",
        "\n",
        "\n",
        "def cvt_data_axis(data):\n",
        "    img = [e[0] for e in data]\n",
        "    qst = [e[1] for e in data]\n",
        "    ans = [e[2] for e in data]\n",
        "    return (img,qst,ans)\n",
        "\n",
        "    \n",
        "def train(epoch, rel, norel):\n",
        "    model.train()\n",
        "\n",
        "    if not len(rel[0]) == len(norel[0]):\n",
        "        print('Not equal length for relation dataset and non-relation dataset.')\n",
        "        return\n",
        "    \n",
        "    random.shuffle(rel)\n",
        "    random.shuffle(norel)\n",
        "\n",
        "    rel = cvt_data_axis(rel)\n",
        "    norel = cvt_data_axis(norel)\n",
        "\n",
        "    for batch_idx in range(len(rel[0]) // bs):\n",
        "        tensor_data(rel, batch_idx)\n",
        "        accuracy_rel = model.train_(input_img, input_qst, label)\n",
        "\n",
        "        tensor_data(norel, batch_idx)\n",
        "        accuracy_norel = model.train_(input_img, input_qst, label)\n",
        "\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)] Relations accuracy: {:.0f}% | Non-relations accuracy: {:.0f}%'.format(epoch, batch_idx * bs * 2, len(rel[0]) * 2, \\\n",
        "                                                                                                                           100. * batch_idx * bs/ len(rel[0]), accuracy_rel, accuracy_norel))\n",
        "            \n",
        "\n",
        "def test(epoch, rel, norel):\n",
        "    model.eval()\n",
        "    if not len(rel[0]) == len(norel[0]):\n",
        "        print('Not equal length for relation dataset and non-relation dataset.')\n",
        "        return\n",
        "    \n",
        "    rel = cvt_data_axis(rel)\n",
        "    norel = cvt_data_axis(norel)\n",
        "\n",
        "    accuracy_rels = []\n",
        "    accuracy_norels = []\n",
        "    for batch_idx in range(len(rel[0]) // bs):\n",
        "        tensor_data(rel, batch_idx)\n",
        "        accuracy_rels.append(model.test_(input_img, input_qst, label)[1])\n",
        "\n",
        "        tensor_data(norel, batch_idx)\n",
        "        accuracy_norels.append(model.test_(input_img, input_qst, label)[1])\n",
        "\n",
        "    accuracy_rel = sum(accuracy_rels) / len(accuracy_rels)\n",
        "    accuracy_norel = sum(accuracy_norels) / len(accuracy_norels)\n",
        "    print('\\n Test set: Relation accuracy: {:.0f}% | Non-relation accuracy: {:.0f}%\\n'.format(\n",
        "        accuracy_rel, accuracy_norel))\n",
        "\n",
        "    \n",
        "def load_data():\n",
        "    print('loading data...')\n",
        "    dirs = './data'\n",
        "    filename = os.path.join(dirs,'sort-of-clevr.pickle')\n",
        "    with open(filename, 'rb') as f:\n",
        "      train_datasets, test_datasets = pickle.load(f)\n",
        "    rel_train = []\n",
        "    rel_test = []\n",
        "    norel_train = []\n",
        "    norel_test = []\n",
        "    print('processing data...')\n",
        "\n",
        "    for img, relations, norelations in train_datasets:\n",
        "        img = np.swapaxes(img,0,2)\n",
        "        for qst,ans in zip(relations[0], relations[1]):\n",
        "            rel_train.append((img,qst,ans))\n",
        "        for qst,ans in zip(norelations[0], norelations[1]):\n",
        "            norel_train.append((img,qst,ans))\n",
        "\n",
        "    for img, relations, norelations in test_datasets:\n",
        "        img = np.swapaxes(img,0,2)\n",
        "        for qst,ans in zip(relations[0], relations[1]):\n",
        "            rel_test.append((img,qst,ans))\n",
        "        for qst,ans in zip(norelations[0], norelations[1]):\n",
        "            norel_test.append((img,qst,ans))\n",
        "    \n",
        "    return (rel_train, rel_test, norel_train, norel_test)\n",
        "    \n",
        "\n",
        "rel_train, rel_test, norel_train, norel_test = load_data()\n",
        "\n",
        "try:\n",
        "    os.makedirs(model_dirs)\n",
        "except:\n",
        "    print('directory {} already exists'.format(model_dirs))\n",
        "\n",
        "if args.resume:\n",
        "    filename = os.path.join(model_dirs, args.resume)\n",
        "    if os.path.isfile(filename):\n",
        "        print('==> loading checkpoint {}'.format(filename))\n",
        "        checkpoint = torch.load(filename)\n",
        "        model.load_state_dict(checkpoint)\n",
        "        print('==> loaded checkpoint {}'.format(filename))\n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    train(epoch, rel_train, norel_train)\n",
        "    test(epoch, rel_test, norel_test)\n",
        "    model.save_model(epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DqzQ7TtTA2mJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Result Visualization**"
      ]
    },
    {
      "metadata": {
        "id": "1J6zMx_pA9ti",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Load the data\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "dirs = './data'\n",
        "filename = os.path.join(dirs,'sort-of-clevr.pickle')\n",
        "with open(filename, 'rb') as f:\n",
        "  train_datasets, test_datasets = pickle.load(f)\n",
        "rel_train = []\n",
        "rel_test = []\n",
        "norel_train = []\n",
        "norel_test = []\n",
        "\n",
        "for img, relations, norelations in train_datasets:\n",
        "    img = np.swapaxes(img,0,2)\n",
        "    for qst,ans in zip(relations[0], relations[1]):\n",
        "        rel_train.append((img,qst,ans))\n",
        "    for qst,ans in zip(norelations[0], norelations[1]):\n",
        "        norel_train.append((img,qst,ans))\n",
        "\n",
        "for img, relations, norelations in test_datasets:\n",
        "    img = np.swapaxes(img,0,2)\n",
        "    for qst,ans in zip(relations[0], relations[1]):\n",
        "        rel_test.append((img,qst,ans))\n",
        "    for qst,ans in zip(norelations[0], norelations[1]):\n",
        "        norel_test.append((img,qst,ans))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "64or4cdyUxqz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "mydict={'batch_size':64,'cuda':True,'epochs':20,'log_interval':10,'lr':0.0001,'model':'RN','no_cuda':False,'resume':None,'seed':1}\n",
        "args = Namespace(**mydict)\n",
        "model=RN(args)\n",
        "model.load_state_dict(torch.load('model/epoch_RN_04.pth'))\n",
        "model.eval();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F97qovgfXePD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "colors = ['red ', 'green ', 'blue ', 'orange ', 'gray ', 'yellow ']\n",
        "answer_sheet = ['yes', 'no', 'rectangle', 'circle', '1', '2', '3', '4', '5', '6']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ye6n2i_NUy6l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "6bb95041-5244-47ab-c2e6-2ff015e09816"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def tensor_data(data, i):\n",
        "    img = torch.from_numpy(np.asarray(data[0][bs*i:bs*(i+1)]))\n",
        "    question = torch.from_numpy(np.asarray(data[1][bs*i:bs*(i+1)]))\n",
        "    answer = torch.from_numpy(np.asarray(data[2][bs*i:bs*(i+1)]))\n",
        "    input_img.data.resize_(img.size()).copy_(img)\n",
        "    input_qst.data.resize_(question.size()).copy_(question)\n",
        "    label.data.resize_(answer.size()).copy_(answer)\n",
        "    \n",
        "def cvt_data_axis(data):\n",
        "    img = [e[0] for e in data]\n",
        "    qst = [e[1] for e in data]\n",
        "    ans = [e[2] for e in data]\n",
        "    return (img,qst,ans)\n",
        "  \n",
        "epoch=0\n",
        "bs=64\n",
        "input_img = torch.FloatTensor(bs, 3, 75, 75)\n",
        "input_qst = torch.FloatTensor(bs, 11)\n",
        "label = torch.LongTensor(bs)\n",
        "\n",
        "model.cuda()\n",
        "input_img = input_img.cuda()\n",
        "input_qst = input_qst.cuda()\n",
        "label = label.cuda()\n",
        "\n",
        "accuracy_rels = []\n",
        "idx=4#index of img,question,answer\n",
        "rel_test2 = cvt_data_axis(rel_test)\n",
        "batch_idx=1\n",
        "# for batch_idx in range(len(rel_test2[0]) // bs):\n",
        "tensor_data(rel_test2, batch_idx);\n",
        "accuracy_rels.append(model.test_(input_img, input_qst, label)[0]);\n",
        "img2 = np.swapaxes(input_img[idx].cpu().detach().numpy(),0,2)\n",
        "plt.imshow(np.dstack((img2[:,:,2],img2[:,:,1],img2[:,:,0])));\n",
        "plt.grid(False)\n",
        "question=input_qst[idx]\n",
        "query = ''\n",
        "query += colors[question.tolist()[0:6].index(1)]+': '\n",
        "if question[6] == 1:\n",
        "    if question[8] == 1:\n",
        "        query += 'shape?'\n",
        "    if question[9] == 1:\n",
        "        query += 'left?'\n",
        "    if question[10] == 1:\n",
        "        query += 'up?'\n",
        "if question[7] == 1:\n",
        "    if question[8] == 1:\n",
        "        query += 'closest shape?'\n",
        "    if question[9] == 1:\n",
        "        query += 'furthest shape?'\n",
        "    if question[10] == 1:\n",
        "        query += 'count?'\n",
        "print(query)    \n",
        "print('Predicted answer',answer_sheet[accuracy_rels[0][idx]])\n",
        "print('Desired answer',answer_sheet[label[idx]])\n",
        "\n",
        "#     break\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "green : closest shape?\n",
            "Predicted answer circle\n",
            "Desired answer circle\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFMCAYAAABCsp4mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}